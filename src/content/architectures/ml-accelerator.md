---
title: "ML Experiment Accelerator"
description: "Notebook experimental ‚Üí pipeline de 5 pasos genera c√≥digo producci√≥n con tests y config MLflow."
domain: "MLOps"
difficulty: "Intermedio"
icon: "üß™"
order: 8
features: ["pipeline", "sub-agents", ".architect.md", "guardrails", "code_rules", "reports"]
---

# ML Experiment Accelerator

> Notebook experimental ‚Üí pipeline de 5 pasos ‚Üí c√≥digo producci√≥n con tests y config MLflow.

## El problema

El 85% de modelos ML nunca llegan a producci√≥n. Un factor clave: el gap entre el notebook del data scientist y el c√≥digo production-ready que necesita el pipeline MLOps. El data scientist sabe qu√© modelo quiere pero no siempre escribe c√≥digo productizable: sin tests, sin type hints, sin manejo de errores, sin logging, con hyperparams hardcodeados.

## D√≥nde encaja architect

Architect como **traductor de notebooks a c√≥digo producci√≥n**. El pipeline toma el notebook experimental, genera c√≥digo limpio siguiendo las convenciones MLOps del equipo (definidas en `.architect.md`), genera tests, y valida que el training pipeline funcione.

## Diagrama

```mermaid
flowchart TD
    A["üë©‚Äçüî¨ Data Scientist\nNotebook experimental\n(experiment.ipynb)"] --> B["architect pipeline\nml-productionize.yaml"]

    subgraph architect_pipeline["Architect Pipeline"]
        direction TB
        B --> C["Step 1: Extract\nParsea notebook,\nextrae l√≥gica core"]
        C --> D["Step 2: Structure\nGenera m√≥dulos:\ndata.py, model.py,\ntrain.py, evaluate.py"]
        D --> E["Step 3: Harden\nA√±ade type hints,\nerror handling,\nlogging, configs"]
        E --> F["Step 4: Test\nGenera tests +\nejecutalos"]
        F --> G["Step 5: Pipeline\nGenera pipeline\nMLflow/Kubeflow YAML"]
    end

    G --> H["PR con:\n- C√≥digo productizado\n- Tests\n- Pipeline config\n- Report"]

    H --> I["ML Engineer Review"]
    I --> J["MLOps Pipeline\n(MLflow / Kubeflow\n/ Vertex AI)"]

    J --> K["Train ‚Üí Evaluate\n‚Üí Register ‚Üí Deploy"]

    style B fill:#2563eb,color:#fff,stroke:#1d4ed8
    style C fill:#7c3aed,color:#fff,stroke:#6d28d9
    style D fill:#7c3aed,color:#fff,stroke:#6d28d9
    style E fill:#7c3aed,color:#fff,stroke:#6d28d9
    style F fill:#7c3aed,color:#fff,stroke:#6d28d9
    style G fill:#7c3aed,color:#fff,stroke:#6d28d9
```

## Implementaci√≥n

### Pipeline YAML

```yaml
# ml-productionize.yaml
name: ml-productionize
steps:
  - name: extract
    agent: build
    task: >
      Parsea experiment.ipynb (formato nbformat JSON).
      Identifica: imports, data loading, preprocessing,
      model definition, training loop, evaluation metrics.
      Ignora: exploratory cells, visualizations, markdown.
      Escribe un resumen en EXTRACTION_PLAN.md.

  - name: structure
    agent: build
    task: >
      Siguiendo EXTRACTION_PLAN.md, genera m√≥dulos Python:
      - src/data/loader.py (data loading + preprocessing)
      - src/models/model.py (model definition)
      - src/training/train.py (training loop con MLflow tracking)
      - src/evaluation/evaluate.py (metrics + evaluation)
      - configs/default.yaml (hyperparams externalizados)

  - name: harden
    agent: build
    task: >
      A√±ade a todos los m√≥dulos generados:
      - Type hints en todas las funciones
      - Docstrings Google-style
      - Logging con structlog (reemplaza prints)
      - Error handling (try/except con mensajes √∫tiles)
      - Seed reproducibility (torch/numpy/random)
      Externaliza TODO hyperparameter a configs/default.yaml.

  - name: test
    agent: build
    task: >
      Genera tests/test_data.py, tests/test_model.py, tests/test_training.py.
      Los tests deben verificar: shapes de datos, forward pass del modelo,
      que el training loop reduce loss en 5 steps.
      Ejecuta pytest para verificar que todos pasan.

  - name: pipeline-config
    agent: build
    task: >
      Genera configs/mlflow_pipeline.yaml con la configuraci√≥n
      para ejecutar el training como job de MLflow:
      entry_points, parameters, metrics, artifacts.
      Genera tambi√©n un Makefile con targets: train, evaluate, test.
```

### .architect.md para ML

```markdown
# ML Code Conventions

## Estructura
- src/data/ ‚Üí data loading, preprocessing, feature engineering
- src/models/ ‚Üí model definitions
- src/training/ ‚Üí training loops, callbacks
- src/evaluation/ ‚Üí metrics, evaluation logic
- configs/ ‚Üí hydra/omegaconf configs
- tests/ ‚Üí pytest tests

## Obligatorio
- Type hints en todas las funciones p√∫blicas
- Docstrings Google-style con Args/Returns/Raises
- Logging con structlog (no print)
- Configs externalizadas (no hardcoded hyperparams)
- Seed reproducibility (torch.manual_seed, np.random.seed)
- MLflow tracking en training loop (log_params, log_metrics, log_model)

## Prohibido
- No imports con * (from x import *)
- No paths absolutos
- No credenciales en c√≥digo
- No dependencias sin versi√≥n fija en requirements.txt
- No globals mutables
```

### Configuraci√≥n

```yaml
# .architect.yaml
llm:
  model: openai/gpt-4.1
  api_key_env: OPENAI_API_KEY

guardrails:
  protected_files:
    - "experiment.ipynb"   # No modificar el notebook original
    - "data/**"            # No tocar datos
    - "*.csv"
    - "*.parquet"
  code_rules:
    - pattern: 'from .* import \*'
      message: "No wildcard imports"
      severity: block
    - pattern: 'print\('
      message: "Usa structlog en vez de print"
      severity: warn
```

## Features de architect usadas

| Feature | Rol en esta arquitectura |
|---------|------------------------|
| **Pipeline** | 5 pasos secuenciales: extract ‚Üí structure ‚Üí harden ‚Üí test ‚Üí config |
| **Sub-agents** | Diferentes agentes para generaci√≥n vs testing |
| **.architect.md** | Convenciones ML del equipo (estructura, logging, configs) |
| **Guardrails** | Protege notebook original y datos |
| **code_rules** | Bloquea wildcards imports, warns en prints |
| **Reports** | Documentaci√≥n de qu√© se gener√≥ y resultados de tests |

## Resultado

De un notebook de 200 celdas, architect genera:
- 4-6 m√≥dulos Python limpios con type hints y docstrings
- Tests unitarios que verifican shapes, forward pass, y convergencia
- Config YAML con hyperparams externalizados
- Pipeline config para MLflow/Kubeflow
- Makefile con targets est√°ndar

El ML Engineer revisa el PR y lo conecta al pipeline MLOps existente. El gap de "semanas de productizaci√≥n" se reduce a "horas de review".

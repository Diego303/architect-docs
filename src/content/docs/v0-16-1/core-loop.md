---
title: "Core Loop"
description: "El AgentLoop: safety nets, StopReason, graceful close, hooks lifecycle."
icon: "M4 4v5h.582m15.356 2A8.001 8.001 0 004.582 9m0 0H9m11 11v-5h-.581m0 0a8.003 8.003 0 01-15.357-2m15.357 2H15"
order: 3
---

# El loop de agente (core/loop.py)

El `AgentLoop` es el coraz√≥n del sistema. Ver tambi√©n [`logging.md`](/architect-docs/docs/v0-16-1/logging) para detalles del sistema de logging.

Usa un bucle `while True` ‚Äî el LLM decide cu√°ndo terminar (deja de pedir tools). Los safety nets (max_steps, budget, timeout, context) son watchdogs que piden un cierre limpio al LLM en lugar de cortar abruptamente.

---

## Pseudoc√≥digo completo (v3)

```python
def run(prompt, stream=False, on_stream_chunk=None):
    # Inicializaci√≥n
    messages = ctx.build_initial(agent_config, prompt)
    tools_schema = registry.get_schemas(agent_config.allowed_tools or None)
    state = AgentState(messages=messages, model=llm.config.model, ...)
    step = 0

    while True:

        # ‚îÄ‚îÄ SAFETY NETS (antes de cada llamada al LLM) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        stop_reason = _check_safety_nets(state, step)
        if stop_reason is not None:
            return _graceful_close(state, stop_reason, tools_schema)

        # ‚îÄ‚îÄ CONTEXT MANAGEMENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if context_manager:
            messages = context_manager.manage(messages, llm)
            # manage() aplica:
            #   1. Compresi√≥n con LLM (si contexto > 75% del m√°ximo)
            #   2. Ventana deslizante hard limit

        # ‚îÄ‚îÄ LLAMADA AL LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        hlog.llm_call(step, messages_count=len(messages))

        try:
            with StepTimeout(step_timeout):
                if stream:
                    response = None
                    for chunk_or_response in llm.completion_stream(messages, tools_schema):
                        if isinstance(chunk_or_response, StreamChunk):
                            if on_stream_chunk:
                                on_stream_chunk(chunk_or_response.data)  # ‚Üí stderr
                        else:
                            response = chunk_or_response  # LLMResponse final

                else:
                    response = llm.completion(messages, tools_schema)

        except StepTimeoutError:
            hlog.step_timeout(step_timeout)
            return _graceful_close(state, StopReason.TIMEOUT, tools_schema)

        except Exception as e:
            hlog.llm_error(str(e))
            state.status = "failed"
            state.stop_reason = StopReason.LLM_ERROR
            state.final_output = f"Error irrecuperable del LLM: {e}"
            return state

        # ‚îÄ‚îÄ REGISTRAR COSTE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if cost_tracker and response.usage:
            try:
                cost_tracker.record(step=step, model=..., usage=response.usage)
            except BudgetExceededError:
                return _graceful_close(state, StopReason.BUDGET_EXCEEDED, tools_schema)

        step += 1

        # ‚îÄ‚îÄ EL LLM DECIDI√ì TERMINAR (no pidi√≥ tools) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if not response.tool_calls:
            hlog.agent_done(step)
            state.final_output = response.content
            state.status = "success"
            state.stop_reason = StopReason.LLM_DONE
            break

        # ‚îÄ‚îÄ EL LLM PIDI√ì TOOLS ‚Üí EJECUTAR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tool_results = _execute_tool_calls_batch(response.tool_calls, step)
        messages = ctx.append_tool_results(messages, response.tool_calls, tool_results)
        state.steps.append(StepResult(step, response, tool_results))

    # ‚îÄ‚îÄ Log final ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    hlog.loop_complete(status=state.status, stop_reason=...,
                       total_steps=state.current_step,
                       total_tool_calls=state.total_tool_calls)
    return state
```

### Diferencia clave con v1

```
ANTES (v1):                         AHORA (v3):

for i in range(max_steps):          while True:
    response = llm(...)                 if watchdog_triggered:
    if done: break                          graceful_close()  ‚Üê LLM resume
    execute_tools()                         break
else:                                   response = llm(...)
    status = "partial"  ‚Üê fr√≠o          if no tool_calls:
                                            done!  ‚Üê LLM decidi√≥
                                            break
                                        execute_tools()
```

El `for-range` hace que `max_steps` sea la estructura. El `while True` hace que **la decisi√≥n del LLM** sea la estructura y `max_steps` sea un guardia.

---

## StopReason ‚Äî por qu√© se detuvo el agente

```python
class StopReason(Enum):
    LLM_DONE = "llm_done"              # El LLM decidi√≥ que termin√≥ (natural)
    MAX_STEPS = "max_steps"            # Watchdog: l√≠mite de pasos
    BUDGET_EXCEEDED = "budget_exceeded" # Watchdog: l√≠mite de coste
    CONTEXT_FULL = "context_full"      # Watchdog: context window lleno
    TIMEOUT = "timeout"                # Watchdog: tiempo total excedido
    USER_INTERRUPT = "user_interrupt"   # El usuario hizo Ctrl+C
    LLM_ERROR = "llm_error"           # Error irrecuperable del LLM
```

`StopReason` se guarda en `AgentState.stop_reason` y se incluye en el JSON output.

---

## Safety nets (`_check_safety_nets`)

Comprueban condiciones antes de cada iteraci√≥n. Si alguna salta, devuelven un `StopReason` y el loop hace `_graceful_close()`.

```python
def _check_safety_nets(state, step) -> StopReason | None:
    # 1. User interrupt (Ctrl+C / SIGTERM) ‚Äî m√°s urgente
    if shutdown and shutdown.should_stop:
        return StopReason.USER_INTERRUPT

    # 2. Max steps ‚Äî watchdog de pasos
    if step >= agent_config.max_steps:
        return StopReason.MAX_STEPS

    # 3. Timeout total ‚Äî watchdog de tiempo
    if timeout and (time.time() - start_time) > timeout:
        return StopReason.TIMEOUT

    # 4. Context window cr√≠ticamente lleno (>95%)
    if context_manager and context_manager.is_critically_full(messages):
        return StopReason.CONTEXT_FULL

    return None  # Todo bien, continuar
```

Cada safety net emite un log HUMAN via `hlog.safety_net()`.

---

## Cierre limpio (`_graceful_close`)

Cuando un safety net salta, no corta abruptamente. Le da al LLM una √∫ltima oportunidad de resumir qu√© hizo y qu√© queda pendiente.

```python
def _graceful_close(state, reason, tools_schema) -> AgentState:
    hlog.closing(reason.value, len(state.steps))

    # USER_INTERRUPT: corte inmediato, sin llamar al LLM
    if reason == StopReason.USER_INTERRUPT:
        state.status = "partial"
        state.final_output = "Interrumpido por el usuario."
        return state

    # Para todos los dem√°s: pedir resumen al LLM
    instruction = _CLOSE_INSTRUCTIONS[reason]
    state.messages.append({"role": "user", "content": f"[SISTEMA] {instruction}"})

    try:
        # √öltima llamada SIN tools ‚Äî solo texto de cierre
        response = llm.completion(messages=state.messages, tools=None)
        state.final_output = response.content
    except Exception:
        state.final_output = f"El agente se detuvo ({reason.value})."

    state.status = "partial"
    state.stop_reason = reason
    hlog.loop_complete(status="partial", ...)
    return state
```

---

## Post-edit hooks (v3-M4)

Despu√©s de que el agente edita un archivo (`edit_file`, `write_file`, `apply_patch`), se ejecutan autom√°ticamente hooks configurados (lint, typecheck, tests). El resultado vuelve al LLM como parte del tool result.

```python
def _execute_single_tool(tc, step) -> ToolCallResult:
    hlog.tool_call(tc.name, tc.arguments)

    result = engine.execute_tool_call(tc.name, tc.arguments)

    # v3-M4: Ejecutar hooks post-edit si aplican
    hook_output = engine.run_post_edit_hooks(tc.name, tc.arguments)

    if hook_output and result.success:
        # A√±adir output de hooks al resultado del tool
        combined_output = result.output + "\n\n" + hook_output
        result = ToolResult(success=result.success, output=combined_output)
        hlog.hook_complete(tc.name)

    hlog.tool_result(tc.name, result.success, result.error)
    return ToolCallResult(tool_name=tc.name, args=tc.arguments, result=result)
```

Ejemplo de output con hooks:
```
   üîß edit_file ‚Üí src/main.py (3‚Üí5 l√≠neas)
      ‚úì OK
      üîç Hook python-lint: ‚úì
```

Si un hook falla, el LLM ve el error y puede auto-corregir:
```
      üîç Hook python-lint: ‚ö†Ô∏è
         src/main.py:45: E302 expected 2 blank lines, found 1
```

### Configuraci√≥n de hooks

```yaml
hooks:
  post_edit:
    - name: python-lint
      command: "ruff check {file} --no-fix"
      file_patterns: ["*.py"]
      timeout: 10

    - name: python-typecheck
      command: "mypy {file}"
      file_patterns: ["*.py"]
      timeout: 15
      enabled: false
```

El placeholder `{file}` se sustituye por el path del archivo editado. La variable de entorno `ARCHITECT_EDITED_FILE` tambi√©n est√° disponible.

---

## Parallel tool calls

Cuando el LLM solicita varias tool calls en un mismo step, el loop puede ejecutarlas en paralelo.

### L√≥gica de decisi√≥n (`_should_parallelize`)

```python
def _should_parallelize(tool_calls) -> bool:
    # Desactivado si el config lo dice
    if context_manager and not context_manager.config.parallel_tools:
        return False

    # confirm-all: siempre secuencial (interacci√≥n con el usuario)
    if agent_config.confirm_mode == "confirm-all":
        return False

    # confirm-sensitive: secuencial si alguna tool es sensible
    if agent_config.confirm_mode == "confirm-sensitive":
        for tc in tool_calls:
            if registry.get(tc.name).sensitive:
                return False

    # yolo o confirm-sensitive sin tools sensibles ‚Üí paralelo
    return True
```

### Implementaci√≥n paralela

```python
def _execute_tool_calls_batch(tool_calls, step):
    if len(tool_calls) <= 1 or not _should_parallelize(tool_calls):
        return [_execute_single_tool(tc, step) for tc in tool_calls]

    # Ejecuci√≥n paralela con ThreadPoolExecutor
    results = [None] * len(tool_calls)
    with ThreadPoolExecutor(max_workers=min(len(tool_calls), 4)) as pool:
        futures = {
            pool.submit(_execute_single_tool, tc, step): i
            for i, tc in enumerate(tool_calls)
        }
        for future in as_completed(futures):
            results[futures[future]] = future.result()
    return results
```

El patr√≥n `{future: idx}` garantiza orden correcto independientemente del orden de completaci√≥n.

---

## ContextManager ‚Äî gesti√≥n del context window

El `ContextManager` act√∫a en tres niveles progresivos para evitar que el contexto se llene en tareas largas.

### Pipeline unificado (`manage`)

```python
def manage(messages, llm=None) -> list[dict]:
    # Solo comprimir si el contexto supera el 75% del m√°ximo
    if llm and _is_above_threshold(messages, 0.75):
        messages = maybe_compress(messages, llm)
    messages = enforce_window(messages)
    return messages
```

El threshold del 75% evita compresiones innecesarias en tareas cortas. Si `max_context_tokens=0` (sin l√≠mite), se conf√≠a en `summarize_after_steps`.

### Nivel 1 ‚Äî Truncado de tool results (`truncate_tool_result`)

Se aplica en `ContextBuilder._format_tool_result()` antes de a√±adir cada tool result al historial.

- `max_tool_result_tokens=0` desactiva el truncado.
- Preserva primeras 40 l√≠neas + √∫ltimas 20 l√≠neas + marcador de omisi√≥n.

### Nivel 2 ‚Äî Compresi√≥n con LLM (`maybe_compress`)

Se activa cuando el n√∫mero de intercambios supera `summarize_after_steps` Y el contexto est√° >75% lleno.

```python
def maybe_compress(messages, llm) -> list[dict]:
    tool_exchanges = _count_tool_exchanges(messages)
    if tool_exchanges <= config.summarize_after_steps:
        return messages  # sin cambios

    old_msgs = dialog[:-keep_count]
    recent_msgs = dialog[-keep_count:]

    # Resumir con el LLM; fallback mec√°nico si falla
    summary = _summarize_steps(old_msgs, llm)

    return [system_msg, user_msg, summary_msg, *recent_msgs]
```

Si el LLM falla al resumir (red, auth, etc.), se genera un resumen mec√°nico (lista de tools y archivos) como fallback.

### Nivel 3 ‚Äî Ventana deslizante (`enforce_window`)

Hard limit que elimina pares de mensajes antiguos hasta que el total estimado cabe.

- `max_context_tokens=0` desactiva el l√≠mite.
- Siempre preserva `messages[0]` (system) y `messages[1]` (user original).

### `is_critically_full` ‚Äî safety net del contexto

```python
def is_critically_full(messages) -> bool:
    # True si el contexto est√° al 95%+ del m√°ximo
    return _estimate_tokens(messages) > int(max_context_tokens * 0.95)
```

Usado como safety net en el loop: si retorna True despu√©s de comprimir, el agente debe cerrar.

### Estimaci√≥n de tokens (`_estimate_tokens`)

```python
def _estimate_tokens(messages) -> int:
    total_chars = 0
    for m in messages:
        if m.get("content"):
            total_chars += len(str(m["content"]))
        for tc in m.get("tool_calls", []):
            total_chars += len(str(tc["function"]["name"]))
            total_chars += len(str(tc["function"]["arguments"]))
        total_chars += 16  # overhead por mensaje
    return total_chars // 4
```

Extrae solo los campos de contenido relevantes (no serializa el dict completo) para evitar sobreestimaciones.

---

## Human logging (v3-M5+M6)

El sistema de logging tiene 3 pipelines:

1. **JSON file** (si configurado) ‚Äî Todo, estructurado
2. **HumanLogHandler** (stderr) ‚Äî Solo eventos de trazabilidad del agente (nivel HUMAN=25)
3. **Console t√©cnico** (stderr) ‚Äî Debug/info controlado por `-v`, excluyendo HUMAN

### Nivel HUMAN

```python
# logging/levels.py
HUMAN = 25  # entre INFO (20) y WARNING (30)
```

### HumanLog ‚Äî helper tipado

El `AgentLoop` usa `self.hlog = HumanLog(logger)` para emitir eventos HUMAN:

```python
hlog.llm_call(step, messages_count)                    # "üîÑ Paso N ‚Üí Llamada al LLM (M mensajes)"
hlog.llm_response(tool_calls)                          # "   ‚úì LLM respondi√≥ con N tool calls"
hlog.tool_call(name, args, is_mcp, mcp_server)        # "   üîß tool ‚Üí summary" or "   üåê tool ‚Üí summary (MCP: server)"
hlog.tool_result(name, success, error)                 # "      ‚úì OK" or "      ‚úó ERROR: ..."
hlog.hook_complete(name, hook, success, detail)        # "      üîç Hook name: ‚úì/‚ö†Ô∏è detail"
hlog.agent_done(step, cost)                            # "‚úÖ Agente completado (N pasos)" + cost
hlog.safety_net(reason, **kw)                          # "‚ö†Ô∏è L√≠mite de pasos alcanzado..."
hlog.closing(reason, steps)                            # "üîÑ Cerrando (reason, N pasos)"
hlog.loop_complete(status, stop_reason, total_steps, total_tool_calls)
hlog.llm_error(error)                                  # "‚ùå Error del LLM: ..."
hlog.step_timeout(seconds)                             # "‚ö†Ô∏è Step timeout (Ns)..."
```

### Formato visual de ejemplo

```
üîÑ Paso 1 ‚Üí Llamada al LLM (3 mensajes)
   ‚úì LLM respondi√≥ con 2 tool calls

   üîß read_file ‚Üí src/main.py
      ‚úì OK
   üîß read_file ‚Üí src/config.py
      ‚úì OK

üîÑ Paso 2 ‚Üí Llamada al LLM (7 mensajes)
   ‚úì LLM respondi√≥ con 1 tool call

   üîß edit_file ‚Üí src/main.py (3‚Üí5 l√≠neas)
      ‚úì OK
      üîç Hook ruff: ‚úì

üîÑ Paso 3 ‚Üí Llamada al LLM (10 mensajes)
   ‚úì LLM respondi√≥ con texto final

‚úÖ Agente completado (3 pasos)
   Raz√≥n: LLM decidi√≥ que termin√≥
  (3 pasos, 3 tool calls)
```

### Args summarizer (M6)

`_summarize_args(tool_name, args)` produce res√∫menes legibles por tool:

| Tool | Ejemplo de resumen |
|------|-------------------|
| `read_file` | `src/main.py` |
| `write_file` | `src/main.py (42 l√≠neas)` |
| `edit_file` | `src/main.py (3‚Üí5 l√≠neas)` |
| `apply_patch` | `src/main.py (+5 -3)` |
| `search_code` | `"validate_path" en src/` |
| `grep` | `"import jwt" en src/` |
| `run_command` | `pytest tests/ -x` |
| MCP tools | primer argumento truncado a 60 chars |

---

## SelfEvaluator ‚Äî auto-evaluaci√≥n del resultado (F12)

Se invoca desde la CLI **despu√©s** de que el agente completa su ejecuci√≥n. Solo eval√∫a estados `"success"`.

### `evaluate_basic` ‚Äî una evaluaci√≥n

El LLM eval√∫a el resultado y responde en JSON: `{"completed": true, "confidence": 0.92, "issues": [], "suggestion": ""}`. Si no pasa, `state.status = "partial"`.

### `evaluate_full` ‚Äî evaluaci√≥n + reintentos

Hasta `max_retries` ciclos de `evaluate_basic()` + `run_fn(correction_prompt)`. Retorna el mejor estado.

### Parseo de respuesta JSON

Tres estrategias en orden:
1. `json.loads(content)` directo.
2. Regex para bloque de c√≥digo JSON.
3. Regex para primer `{...}`.

---

## Estado del loop (AgentState)

```
AgentState
‚îú‚îÄ‚îÄ messages: list[dict]           ‚Üê historial OpenAI (gestionado por ContextManager)
‚îú‚îÄ‚îÄ steps: list[StepResult]        ‚Üê resultados inmutables de cada step
‚îú‚îÄ‚îÄ status: str                    ‚Üê "running" | "success" | "partial" | "failed"
‚îú‚îÄ‚îÄ stop_reason: StopReason | None ‚Üê por qu√© se detuvo
‚îú‚îÄ‚îÄ final_output: str | None       ‚Üê respuesta final del agente
‚îú‚îÄ‚îÄ start_time: float              ‚Üê para calcular duration_seconds
‚îú‚îÄ‚îÄ model: str | None              ‚Üê modelo usado
‚îî‚îÄ‚îÄ cost_tracker: CostTracker | None ‚Üê F14: tracker de costes
```

Transiciones de estado (v3):

```
                  tool_calls
"running" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí "running" (siguiente step)
    ‚îÇ
    ‚îÇ  no tool_calls (LLM decidi√≥ terminar)
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí "success" (StopReason.LLM_DONE)
    ‚îÇ                               ‚îÇ
    ‚îÇ                               ‚îÇ SelfEvaluator (b√°sico, falla)
    ‚îÇ                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí "partial"
    ‚îÇ
    ‚îÇ  safety net: MAX_STEPS
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí _graceful_close ‚Üí "partial"
    ‚îÇ                            (LLM resume qu√© hizo)
    ‚îÇ
    ‚îÇ  safety net: BUDGET_EXCEEDED
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí _graceful_close ‚Üí "partial"
    ‚îÇ
    ‚îÇ  safety net: TIMEOUT / CONTEXT_FULL
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí _graceful_close ‚Üí "partial"
    ‚îÇ
    ‚îÇ  safety net: USER_INTERRUPT
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí "partial" (corte inmediato, sin LLM)
    ‚îÇ
    ‚îÇ  LLM Exception
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí "failed" (StopReason.LLM_ERROR)
```

---

## Acumulaci√≥n de mensajes (ContextBuilder)

Cada step a√±ade mensajes. El historial (o la versi√≥n comprimida) se env√≠a al LLM en cada llamada.

```
Paso 0 (inicial):
messages = [
  {"role": "system",    "content": "Eres un agente de build...\n\n## Estructura del Proyecto\n..."},
  {"role": "user",      "content": "refactoriza main.py"}
]

Despu√©s de tool calls en step 1 (con truncado Nivel 1):
messages = [
  {"role": "system",    "content": "..."},
  {"role": "user",      "content": "refactoriza main.py"},
  {"role": "assistant", "tool_calls": [...]},
  {"role": "tool",      "content": "def foo():\n    pass\n...\n[... 120 l√≠neas omitidas ...]\n..."}
]

Despu√©s de 9+ steps (con compresi√≥n Nivel 2, si contexto > 75%):
messages = [
  {"role": "system",    "content": "..."},
  {"role": "user",      "content": "refactoriza main.py"},
  {"role": "assistant", "content": "[Resumen de pasos anteriores]\nEl agente ley√≥ main.py, ..."},
  ... (√∫ltimos 4 steps completos) ...
]
```

---

## Streaming

Cuando `stream=True`:
1. `llm.completion_stream(messages, tools)` devuelve un generator.
2. Cada `StreamChunk` tiene `type="content"` y `data=str`.
3. El loop llama a `on_stream_chunk(chunk.data)` ‚Äî escribe a `stderr`.
4. El √∫ltimo item es un `LLMResponse` completo (con `tool_calls` si los hay).
5. Los chunks de tool calls **no** se env√≠an al callback.

El streaming se desactiva autom√°ticamente en: fase plan del modo mixto, `--json`, `--quiet`, `--no-stream`, reintentos de `evaluate_full`.

---

## Shutdown graceful (GracefulShutdown)

```
GracefulShutdown
‚îú‚îÄ‚îÄ __init__: instala handler en SIGINT + SIGTERM
‚îú‚îÄ‚îÄ _handler(signum):
‚îÇ     1er disparo ‚Üí _interrupted=True, avisa en stderr
‚îÇ     2do disparo SIGINT ‚Üí sys.exit(130) inmediato
‚îî‚îÄ‚îÄ should_stop: property ‚Üí _interrupted
```

El loop comprueba `shutdown.should_stop` en `_check_safety_nets()` al inicio de cada iteraci√≥n. Si True, `_graceful_close()` corta inmediatamente (USER_INTERRUPT no llama al LLM).

---

## Timeout por step (StepTimeout)

```python
with StepTimeout(60):          # 60 segundos
    response = llm.completion(...)
# Si tarda > 60s: SIGALRM ‚Üí StepTimeoutError ‚Üí _graceful_close(TIMEOUT)
```

- Solo activo en Linux/macOS (usa `SIGALRM`). En Windows: no-op.
- `step_timeout` viene del flag `--timeout` de CLI.

---

## Par√°metros del constructor

```python
AgentLoop(
    llm:             LLMAdapter,
    engine:          ExecutionEngine,
    agent_config:    AgentConfig,
    ctx:             ContextBuilder,
    shutdown:        GracefulShutdown | None = None,
    step_timeout:    int = 0,                        # 0 = sin timeout
    context_manager: ContextManager | None = None,
    cost_tracker:    CostTracker | None = None,      # F14: tracking de costes
    timeout:         int | None = None,              # timeout total de ejecuci√≥n
)
```

El loop no crea sus dependencias ‚Äî las recibe como par√°metros (inyecci√≥n de dependencias). Internamente crea `self.hlog = HumanLog(logger)` para emitir logs de trazabilidad.

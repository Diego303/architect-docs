---
title: "Evaluaci√≥n Competitiva"
description: "Comparaci√≥n multi-modelo con ranking por calidad, eficiencia y coste."
icon: "M3 6l3 1m0 0l-3 9a5.002 5.002 0 006.001 0M6 7l3 9M6 7l6-2m6 2l3-1m-3 1l-3 9a5.002 5.002 0 006.001 0M18 7l3 9m-3-9l-6-2m0-2v2m0 16V5m0 16H9m3 0h3"
order: 27
---

# Evaluaci√≥n Competitiva (Competitive Eval)

Comparaci√≥n automatizada de m√∫ltiples modelos LLM ejecutando la misma tarea, con ranking basado en calidad, eficiencia y coste.

Implementado en `src/architect/features/competitive.py`. Disponible desde v1.0.0 (Plan base v4 Phase D ‚Äî D3).

---

## Concepto

`architect eval` ejecuta la misma tarea con m√∫ltiples modelos en paralelo (cada uno en un git worktree aislado) y despu√©s corre los mismos checks de validaci√≥n en cada worktree. Genera un ranking comparativo basado en un score compuesto.

```bash
architect eval "implementa autenticaci√≥n JWT" \
  --models gpt-4o,claude-sonnet-4-6,gemini-2.0-flash \
  --check "pytest tests/test_auth.py -q" \
  --check "ruff check src/" \
  --budget-per-model 1.0
```

---

## C√≥mo funciona

```
architect eval TASK --models m1,m2,m3 --check "cmd1" --check "cmd2"
  ‚îÇ
  ‚îú‚îÄ‚îÄ Crear CompetitiveConfig
  ‚îÇ     ‚îî‚îÄ‚îÄ task, models, checks, agent, max_steps, budget, timeout
  ‚îÇ
  ‚îú‚îÄ‚îÄ CompetitiveEval.run()
  ‚îÇ     ‚îú‚îÄ‚îÄ ParallelRunner (reutiliza infraestructura de parallel)
  ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ Cada modelo ‚Üí git worktree ‚Üí `architect run` como subprocess
  ‚îÇ     ‚îÇ
  ‚îÇ     ‚îú‚îÄ‚îÄ Para cada worktree resultante:
  ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ _run_checks_in_worktree(checks) ‚Üí (passed, total)
  ‚îÇ     ‚îÇ
  ‚îÇ     ‚îî‚îÄ‚îÄ _rank_results() ‚Üí calcular score compuesto
  ‚îÇ
  ‚îú‚îÄ‚îÄ CompetitiveEval.generate_report()
  ‚îÇ     ‚îî‚îÄ‚îÄ Tabla markdown con ranking
  ‚îÇ
  ‚îî‚îÄ‚îÄ Mostrar reporte (stdout o --report-file)
```

---

## Sistema de puntuaci√≥n

El score compuesto es sobre **100 puntos**:

| Componente | Peso | C√°lculo |
|------------|------|---------|
| Checks pasados | 40 pts | `(checks_passed / checks_total) * 40` |
| Status | 30 pts | success=30, partial=15, timeout=5, failed=0 |
| Eficiencia | 20 pts | Menos pasos = mayor puntuaci√≥n (normalizado) |
| Coste | 10 pts | Menor coste = mayor puntuaci√≥n (normalizado) |

---

## CLI

```
architect eval PROMPT [opciones]
```

### Opciones

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--models LIST` | Modelos separados por coma (requerido) |
| `--check CMD` | Comando de verificaci√≥n (repetible, requerido) |
| `--agent NAME` | Agente a usar (default: `build`) |
| `--max-steps N` | M√°ximo de pasos por modelo (default: 50) |
| `--budget-per-model N` | L√≠mite de coste por modelo en USD |
| `--timeout-per-model N` | L√≠mite de tiempo por modelo en segundos |
| `--report-file PATH` | Guardar reporte en archivo |
| `--config PATH` | Archivo de configuraci√≥n YAML |
| `--api-base URL` | URL base de la API LLM |

### Ejemplos

```bash
# Comparar 3 modelos con checks
architect eval "refactoriza utils.py" \
  --models gpt-4o,claude-sonnet-4-6,deepseek-chat \
  --check "pytest tests/ -q" \
  --check "ruff check src/" \
  --budget-per-model 0.50

# Guardar reporte
architect eval "optimiza queries SQL" \
  --models gpt-4o,claude-sonnet-4-6 \
  --check "pytest" \
  --report-file eval_report.md

# Con timeout estricto
architect eval "implementa feature" \
  --models gpt-4o-mini,claude-sonnet-4-6 \
  --check "pytest tests/" \
  --timeout-per-model 300 \
  --max-steps 30
```

---

## API

### `CompetitiveConfig`

```python
@dataclass
class CompetitiveConfig:
    task: str
    models: list[str]
    checks: list[str]
    agent: str = "build"
    max_steps: int = 50
    budget_per_model: float | None = None
    timeout_per_model: int | None = None
    config_path: str | None = None
    api_base: str | None = None
```

### `CompetitiveResult`

```python
@dataclass
class CompetitiveResult:
    model: str
    status: str               # success | partial | failed | timeout
    steps: int
    cost: float
    duration: float
    files_modified: list[str]
    checks_passed: int
    checks_total: int
    worktree_path: str
    score: float              # score compuesto (0-100)
```

### `CompetitiveEval`

```python
class CompetitiveEval:
    def __init__(self, config: CompetitiveConfig, workspace_root: str): ...
    def run(self) -> list[CompetitiveResult]: ...
    def generate_report(self, results: list[CompetitiveResult]) -> str: ...
```

---

## Reporte generado

El reporte incluye:

1. **Tabla comparativa**: modelo, status, pasos, coste, tiempo, checks pasados, archivos modificados
2. **Ranking**: ordenado por score compuesto (1er, 2do, 3er lugar)
3. **Resultados de checks**: detalle por modelo
4. **Worktree paths**: para inspecci√≥n manual de cada resultado

```markdown
## Ranking

| # | Modelo | Score | Status | Steps | Cost | Checks |
|---|--------|-------|--------|-------|------|--------|
| 1 | gpt-4o | 85.0 | success | 12 | $0.42 | 3/3 |
| 2 | claude-sonnet-4-6 | 78.5 | success | 15 | $0.38 | 2/3 |
| 3 | deepseek-chat | 45.0 | partial | 30 | $0.12 | 1/3 |
```

---

## Salida visual (HUMAN logging)

A partir de v1.1.0, la evaluaci√≥n competitiva emite eventos de nivel HUMAN que producen una salida visual clara en stderr. El usuario puede ver el ranking en tiempo real sin necesidad de flags `-v`.

```
   üèÜ gpt-4.1: #1 (score: 85, 5/5 checks, $0.0456)
   ü•à claude-sonnet: #2 (score: 72, 4/5 checks, $0.0312)
   ü•â gpt-4.1-mini: #3 (score: 41, 2/5 checks, $0.0089)

üèÅ Ranking final: gpt-4.1 > claude-sonnet > gpt-4.1-mini
```

Los modelos en posiciones 1, 2 y 3 muestran medallas (üèÜ, ü•à, ü•â). A partir de la 4¬™ posici√≥n se muestra `#N`.

### Eventos emitidos

| Evento | Cu√°ndo | Datos |
|--------|--------|-------|
| `competitive.model_done` | Al completar cada modelo | model, rank, score, cost, checks_passed, checks_total |
| `competitive.ranking` | Al finalizar la evaluaci√≥n | ranking (lista de {model, score, rank}) |

Se desactiva con `--quiet` o `--json`. Ver [`logging.md`](/architect-docs/docs/v1-1-0/logging) para detalles del sistema HUMAN.

---

## Relaci√≥n con Parallel

`CompetitiveEval` reutiliza la infraestructura de `ParallelRunner` (git worktrees + ProcessPoolExecutor). La diferencia es que:

- `parallel` ejecuta **tareas diferentes** (o la misma tarea) con posiblemente diferentes modelos
- `eval` ejecuta la **misma tarea** con **diferentes modelos** y a√±ade validaci√≥n con checks + ranking

---

## Archivos

| Archivo | Contenido |
|---------|-----------|
| `src/architect/features/competitive.py` | `CompetitiveEval`, `CompetitiveConfig`, `CompetitiveResult` |
| `src/architect/cli.py` | Comando `architect eval` |
| `tests/test_competitive/` | Tests unitarios |
